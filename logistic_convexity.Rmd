---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# What's wrong with sum of squares for logistic regression?

```{python tags=c("hide-cell")}
import numpy as np
import pandas as pd
# Safe settings for Pandas.
pd.set_option('mode.chained_assignment', 'raise')
# %matplotlib inline
import matplotlib.pyplot as plt
# Make the plots look more fancy.
plt.style.use('fivethirtyeight')
# Optimization function
from scipy.optimize import minimize
```

This page gives some extra explanation for the
[logistic_regression](logistic_regression) page.

In that page we were trying to looking at the [chronic kidney disease
dataset](../data/chronic_kidney_disease), to see if we good predict whether a
patient had "good" appetite (as opposed to "poor" appetite) given that patient's blood hemoglobin concentration.

```{python tags=c("hide-cell")}
df = pd.read_csv('ckd_clean.csv')
# Our columns of interest.
hgb_app = df.loc[:, ['Hemoglobin', 'Appetite']].copy()
# Dummy value column containing 0 for "poor" Appetite, 1 for "good".
good_appetite = hgb_app['Appetite'] == 'good'
# Convert to an integer (1 for True, 0 for False), and store.
hgb_app['appetite_dummy'] = good_appetite.astype(int)
```

We take out the columns we are interested in for our further use:

```{python}
# The x (predictor) and y (to-be-predicted) variables.
hemoglobin = hgb_app['Hemoglobin']
appetite_d = hgb_app['appetite_dummy']
```

Here is a plot of the 0 / 1 appetite values

```{python tags=c("hide-cell")}
def plot_hgb_app():
    # Build plot, add custom label.
    # Series to provide colors.
    colors = appetite_d.replace(0, 'red').replace(1, 'blue')
    hgb_app.plot.scatter('Hemoglobin', 'appetite_dummy', c=colors)
    plt.ylabel('Appetite\n0 = poor, 1 = good')
    plt.yticks([0,1]);  # Just label 0 and 1 on the y axis.
    # Put a custom legend on the plot.  This code is a little obscure.
    plt.scatter([], [], c='blue', label='good')
    plt.scatter([], [], c='red', label='poor')

# Do the plot
plot_hgb_app()
# Show the legend
plt.legend();
```

We defined the functions to convert between the straight line predictions and the sigmoid predictions:

```{python}
def inv_logit(y):
    """ Reverse logit transformation
    """
    odds_ratios = np.exp(y)  # Reverse the log operation.
    return odds_ratios / (odds_ratios + 1)  # Reverse odds ratios operation.


def params2pps(intercept, slope, x):
    """ Calculate predicted probabilities of 1 for each observation.
    """
    # Predicted log odds of being in class 1.
    predicted_log_odds = intercept + slope * x
    return inv_logit(predicted_log_odds)
```

This allowed us to build our sum of squares logit cost function:


```{python}
def ss_logit(c_s, x_values, y_values):
    # Unpack intercept and slope into values.
    intercept, slope = c_s
    # Predicted p values on sigmoid
    pps = params2pps(intercept, slope, x_values)
    # Prediction errors.
    sigmoid_error = y_values - pps
    # Sum of squared error
    return np.sum(sigmoid_error ** 2)
```

Then we found our sum of squares best straight line (that corresponds to a sigmoid after transformation):

```{python}
min_res_logit = minimize(ss_logit, [-7, 0.8], args=(hemoglobin, appetite_d))
min_res_logit
```

Calculate the predicted 0 / 1 labels.

```{python}
logit_ss_inter, logit_ss_slope = min_res_logit.x
logit_ss_pps = params2pps(logit_ss_inter, logit_ss_slope, hemoglobin)
```

Plot them:

```{python tags=c("hide-cell")}
plot_hgb_app()
# A new plot on top of the old.
plt.scatter(hemoglobin, logit_ss_pps,
            label='Logit ss solution',
            color='gold')
# Show the legend.
plt.legend();
```

Our sigmoid prediction from sum of squares above looks convincing enough, but, is there a better way of scoring our lines, than sum of squares?

If you want the gory details, see [this
answer on StackOverflow](https://stats.stackexchange.com/a/254067). Here we
look at whether this the sum of squares works well with minimize.  The
discussion here corresponds to the "computational efficiency" section of the answer.

In other sections of this textbook, you have calculated the sum of squares
error for a range of slopes and intercepts.  In the simple cases we were
looking at there, we found a very pleasant looking symmetrical parabola; when
the intercept or slope was far from the optimal, the sum of squared error was
very high, as we try intercepts (or slopes) nearer the optimal, the error comes
down quickly, to a clear minimum.  That is the ideal situation for `minimize`, but we don't have the same situation here.

In the next cell, we set the slope to the best sum of squares slope minimize
found, and vary the intercept, calculating the error for different intercepts.
There is a clear minimum, but we have lost the nice parabola shape.  For
intercepts greater than about 5, the graph is very flat, so minimize can ran
into trouble if it is trying values of about - say - 10 - because smallish
changes in the intercept will make very little difference to the error.

```{python}
intercepts = np.linspace(-10, 5, 1000)
n = len(intercepts)
ss_errors = np.zeros(n)
for i in np.arange(n):
    ss_errors[i] = ss_logit([intercepts[i], logit_ss_slope],
                            hemoglobin, appetite_d)

plt.scatter(intercepts, ss_errors)
plt.xlabel('intercept')
plt.ylabel('Sigmoid SS error')
```

It can get even worse when trying slopes that are further away from the
optimum.  Here it is very difficult to see the minimum for the error, when we have the slope badly wrong at -1:

```{python}
slopes = np.linspace(0, 1, 1000)
n = len(slopes)
ss_errors = np.zeros(n)
for i in np.arange(n):
    ss_errors[i] = ss_logit([logit_ss_inter, slopes[i]],
                            hemoglobin, appetite_d)

plt.scatter(intercepts, ss_errors)
plt.xlabel('slope')
plt.ylabel('Sigmoid SS error')
```
