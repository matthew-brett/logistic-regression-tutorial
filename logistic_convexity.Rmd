---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# What's wrong with sum of squares for logistic regression?

```{python tags=c("hide-cell")}
import numpy as np
import pandas as pd
# Safe settings for Pandas.
pd.set_option('mode.chained_assignment', 'raise')
# %matplotlib inline
import matplotlib.pyplot as plt
# Make the plots look more fancy.
plt.style.use('fivethirtyeight')
# Optimization function
from scipy.optimize import minimize
```

This page gives some extra explanation for the
[logistic_regression](logistic_regression) page.

Here we say more about why we might prefer the Maximum Likelihood Estimate way
of scoring potential fits to the data, to our more usual least squared error.
If you want the gory details on this choice, see [this answer on
StackOverflow](https://stats.stackexchange.com/a/254067). Here we look at
whether this the sum of squares works well with minimize.  The discussion in
this page corresponds to the "computational efficiency" section of the answer
linked above.

## The background, the data

In that page we were trying to looking at the [chronic kidney disease
dataset](../data/chronic_kidney_disease), to see if we good predict whether a
patient had "good" appetite (as opposed to "poor" appetite) given that patient's blood hemoglobin concentration.

```{python tags=c("hide-cell")}
df = pd.read_csv('ckd_clean.csv')
# Our columns of interest.
hgb_app = df.loc[:, ['Hemoglobin', 'Appetite']].copy()
# Dummy value column containing 0 for "poor" Appetite, 1 for "good".
good_appetite = hgb_app['Appetite'] == 'good'
# Convert to an integer (1 for True, 0 for False), and store.
hgb_app['appetite_dummy'] = good_appetite.astype(int)
```

We take out the columns we are interested in for our further use:

```{python}
# The x (predictor) and y (to-be-predicted) variables.
hemoglobin = hgb_app['Hemoglobin']
appetite_d = hgb_app['appetite_dummy']
```

Here is a plot of the 0 / 1 appetite values

```{python tags=c("hide-cell")}
def plot_hgb_app():
    # Build plot, add custom label.
    # Series to provide colors.
    colors = appetite_d.replace(0, 'red').replace(1, 'blue')
    hgb_app.plot.scatter('Hemoglobin', 'appetite_dummy', c=colors)
    plt.ylabel('Appetite\n0 = poor, 1 = good')
    plt.yticks([0,1]);  # Just label 0 and 1 on the y axis.
    # Put a custom legend on the plot.  This code is a little obscure.
    plt.scatter([], [], c='blue', label='good')
    plt.scatter([], [], c='red', label='poor')

# Do the plot
plot_hgb_app()
# Show the legend
plt.legend();
```

## Linear regression - the crude approach

The crude and brutal approach to predicting these values is to use simple
least-squares regression.   We can do this in the usual way by using
`scipy.optimize.minimize` with a function that returns the sum of squared
error between the straight line predictions and the 0 / 1 labels.  Here's the function:

```{python}
def ss_any_line(c_s, x_values, y_values):
    """ Cost function for sum of squares prediction error
    """
    # c_s is a list containing two elements, an intercept and a slope.
    intercept, slope = c_s
    # Values predicted from these x_values, using this intercept and slope.
    predicted = intercept + x_values * slope
    # Difference of prediction from the actual y values.
    error = y_values - predicted
    # Sum of squared error.
    return np.sum(error ** 2)
```

Start with a guess of intercept -0.5, slope 0.1:

```{python}
# Use cost function with miminize.
min_res_ss = minimize(ss_any_line, [-0.5, 0.1], args=(hemoglobin, appetite_d))
min_res_ss
```

Store the slope and intercept:

```{python}
inter_ss, slope_ss = min_res_ss.x
```

Show the results:

```{python tags=c("hide-cell")}
# Do the base plot of the hemoglobin and appetite_d.
plot_hgb_app()

# A new plot on top of the old.
predicted_ss = inter_ss + slope_ss * hemoglobin
plt.scatter(hemoglobin, predicted_ss,
            label='LR prediction',
            color='orange')
plt.title("Linear regression with sum of squares")
# Show the legend.
plt.legend();
```

Let us remind ourselves of how the sum of square error values change as we change the slope and the intercept.  First we hold the slope constant at a fairly bad guess of 0.1, and try different intercepts.  For each intercept we calculate the sum of squared error:

```{python}
# Intercepts to try.
intercepts = np.linspace(-2, 2, 1000)
n_inters = len(intercepts)
ss_errors = np.zeros(n_inters)
for i in np.arange(n_inters):
    ss_errors[i] = ss_any_line([intercepts[i], 0.1], hemoglobin, appetite_d)
plt.scatter(intercepts, ss_errors)
plt.xlabel('intercept')
plt.ylabel('Linear SS error')
plt.title("Errors for different intercepts, slope 0.1')
```

Notice the very simple shape of this curve.  It is a parabola, it descends
steeply for values far from the minimum, and more slowly as it gets closer.
This is a curve that `minimize` finds it very easy to work with, because every
time it tries an intercept (in this case) the slope suggest what direction to
go next, and this suggestion is always correct.  You may also have noticed
that this parabola shape is always the same for these simple least squares
functions, like `ss_any_line`.

Just to illustrate again, here we try holding the intercept constant at a fairly bad guess of 0.5, and vary the slopes.  Notice we get the same helpful parabola shape:

```{python}
# Slopes to try.
slopes = np.linspace(0, 0.5, 1000)
n_slopes = len(slopes)
ss_errors = np.zeros(n_slopes)
for i in np.arange(n_slopes):
    ss_errors[i] = ss_any_line([0.5, slopes[i]], hemoglobin, appetite_d)
plt.scatter(slopes, ss_errors)
plt.xlabel('slope')
plt.ylabel('Linear SS error')
plt.title("Errors for different slopes, intercept 0.5')
```

These are plots of how the value of the cost function changes as we change the
parameters.  The curves we see above are parabolas, and these are examples of
curves that are [convex](https://en.wikipedia.org/wiki/Convex_function);
convex curves like parabolas are particularly easy and quick for `minimize` to
work with.

We will see that using sum of squared error with our preferred sigmoid
prediction curve generates cost function curves that are a lot more
complicated, making it more difficult for `minimize` to find the best
parameters.


## More sophisticated - the logistic fit with least squares

For the reasons you saw in the [logistic regression page](logistic
regression), we recoil from the very simple straight line fit above, and
prefer to use a sigmoid curve to fit the 0 / 1 labels.

We defined the functions to convert between the straight line predictions that
we want to use with `minimize` and the sigmoid predictions:

```{python}
def inv_logit(y):
    """ Reverse logit transformation
    """
    odds_ratios = np.exp(y)  # Reverse the log operation.
    return odds_ratios / (odds_ratios + 1)  # Reverse odds ratios operation.


def params2pps(intercept, slope, x):
    """ Calculate predicted probabilities of 1 for each observation.
    """
    # Predicted log odds of being in class 1.
    predicted_log_odds = intercept + slope * x
    return inv_logit(predicted_log_odds)
```

This allowed us to build our sum of squares logit cost function:

```{python}
def ss_logit(c_s, x_values, y_values):
    # Unpack intercept and slope into values.
    intercept, slope = c_s
    # Predicted p values on sigmoid
    pps = params2pps(intercept, slope, x_values)
    # Prediction errors.
    sigmoid_error = y_values - pps
    # Sum of squared error
    return np.sum(sigmoid_error ** 2)
```

Then we found our sum of squares best straight line (that corresponds to a
sigmoid after transformation):

```{python}
min_res_logit = minimize(ss_logit, [-7, 0.8], args=(hemoglobin, appetite_d))
min_res_logit
```

Calculate the predicted 0 / 1 labels.

```{python}
logit_ss_inter, logit_ss_slope = min_res_logit.x
logit_ss_pps = params2pps(logit_ss_inter, logit_ss_slope, hemoglobin)
```

Plot them:

```{python tags=c("hide-cell")}
plot_hgb_app()
# A new plot on top of the old.
plt.scatter(hemoglobin, logit_ss_pps,
            label='Logit ss solution',
            color='gold')
# Show the legend.
plt.legend();
```

In other sections of this textbook, you have calculated the sum of squares
error for a range of slopes and intercepts.  In the simple cases we were
looking at there, we found a very pleasant looking symmetrical parabola; when
the intercept or slope was far from the optimal, the sum of squared error was
very high, as we try intercepts (or slopes) nearer the optimal, the error comes
down quickly, to a clear minimum.  That is the ideal situation for `minimize`, but we don't have the same situation here.

In the next cell, we set the slope to the best sum of squares slope minimize
found, and vary the intercept, calculating the error for different intercepts.
There is a clear minimum, but we have lost the nice parabola shape.  For
intercepts greater than about 5, the graph is very flat, so minimize can ran
into trouble if it is trying values of about - say - 10 - because smallish
changes in the intercept will make very little difference to the error.

```{python}
intercepts = np.linspace(-10, 5, 1000)
n = len(intercepts)
ss_errors = np.zeros(n)
for i in np.arange(n):
    ss_errors[i] = ss_logit([intercepts[i], logit_ss_slope],
                            hemoglobin, appetite_d)

plt.scatter(intercepts, ss_errors)
plt.xlabel('intercept')
plt.ylabel('Sigmoid SS error')
```

It can get even worse when trying slopes that are further away from the
optimum.  Here it is very difficult to see the minimum for the error, when we have the slope badly wrong at -1:

```{python}
slopes = np.linspace(0, 1, 1000)
n = len(slopes)
ss_errors = np.zeros(n)
for i in np.arange(n):
    ss_errors[i] = ss_logit([logit_ss_inter, slopes[i]],
                            hemoglobin, appetite_d)

plt.scatter(intercepts, ss_errors)
plt.xlabel('slope')
plt.ylabel('Sigmoid SS error')
```
