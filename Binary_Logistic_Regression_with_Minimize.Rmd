---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Logistic Regression

```{python tags=c("hide-cell")}
import numpy as np
import pandas as pd
# Safe settings for Pandas.
pd.set_option('mode.chained_assignment', 'raise')
# %matplotlib inline
import matplotlib.pyplot as plt
# Make the plots look more fancy.
plt.style.use('fivethirtyeight')
# Optimization function
from scipy.optimize import minimize
```

By Peter Rush and Matthew Brett, with considerable inspiration from the
logistic regression section of Allen Downey's book [Think Stats, second
edition](https://greenteapress.com/thinkstats2).

In this section we will look at another regression technique: logistic
regression.

We use logistic regression when we want to predict a *binary categorical*
outcome variable (or column) from one or more predicting variables (or
columns).

A binary categorical variable is one where an observation can fall into one of
only two categories. We give each observation a label corresponding to their
category.  Some examples are:

* Did a patient die or did they survive through 6 months of treatment?  The
  patient can only be in only one of the categories.  In some column of our
  data table, patients that died might have the label "died", and those who
  have survived have the label "survived".
* Did a person experience more than one episode of psychosis in the last 5
  years ("yes" or "no")?
* Did a person with a conviction for one offense offend again ("yes" or "no")?

For this tutorial, we return to the [chronic kidney disease
dataset](../data/chronic_kidney_disease).

Each row in this dataset represents one patient.

For each patient, the doctors recorded whether or not the patient had chronic
kidney disease. This is a *binary categorical variable*; you can see the
values in the "Class" column. A value of 1 means the patient *did* have CKD; a
value of 0 means they *did not*.  In this case we are labeling the categories
with numbers (1 / 0).

Many of the rest of the columns are measurements from blood tests and urine tests.

```{python}
df = pd.read_csv('ckd_clean.csv')
df.head()
```

There are actually a large number of binary categorical variables in this
dataset.   For example, the "Hypertension" column has labels for the two
categories "yes" (the patient had persistently high blood pressure) or "no".

The categorical variable we are interested in here is "Appetite".  This has
the label "good" for patients with good appetite, and "poor" for those with
poor appetite.  Poor appetite is a [symptom of chronic kidney
disease](https://www.sciencedirect.com/science/article/abs/pii/S0270929508001666).  In our case, we wonder whether the extent of kidney damage does a convincing job in predicting whether the patient has a "good" appetite.

As you remember, the CKD dataset has a column "Hemoglobin" that has the
concentration of hemoglobin from a blood sample.  Hemoglobin is the molecule
that carries oxygen in the blood; it is the molecule that makes the red blood
cells red.  Damaged kidneys produce lower concentrations of the hormone that
stimulates red blood cell production,
[erythropoietin](https://en.wikipedia.org/wiki/Erythropoietin), so CKD
patients often have fewer red blood cells, and lower concentrations of
Hemoglobin.  We will take lower "Hemoglobin" as a index of kidney damage.
Therefore, we predict that patients with lower "Hemoglobin" values are more
likely to have `poor` "Appetite" values, and, conversely, patients with higher
"Hemoglobin" values are more likely to have `good` "Appetite" values.

First we make a new data frame that just has the two columns we are interested
in:

```{python}
hgb_app = df.loc[:, ['Hemoglobin', 'Appetite']].copy()
hgb_app.head()
```

## Dummy Variables

We will soon find ourselves wanting to do calculations on the values in the
"Appetite" column, and we cannot easily do that with the current string values
of "good" and "poor".   Our next step is to recode the string values to
numbers, ready for our calculations.  We use 1 to represent "good" and 0 to
represent "poor".  This kind of recoding, where we replace category labels
with 1 and 0 values, is often called *dummy coding*.

```{python}
# Make a Boolean Series (True, False)
good_appetite = hgb_app['Appetite'] == 'good'
# Convert to an integer (1 for True, 0 for False), and store.
hgb_app['appetite_dummy'] = good_appetite.astype(int)
hgb_app.head()
```

*Note*: When you are doing this, be sure to keep track of which label you have
coded as 1. Normally this would be the more interesting outcome.  In this
case, "good" has the code 1. Keep track of the label corresponding to 1, as it
will affect the interpretation of the regression coefficients.

Now we have the dummy (1 or 0) variable, let us use a scatter plot to look at
the relationship between hemoglobin concentration and whether a patient has a
good appetite.

Before we do this, it is convenient to add another column to color the "good" and "poor" appetite points:

```{python}
hgb_app['appetite_color'] = 'red'
hgb_app.loc[good_appetite, 'appetite_color'] = 'blue'
```

Now we can plot the hemoglobin values on the x axis (our predictor) against
the appetite values on the y axis (the values to predict).

```{python tags=c("hide-cell")}
# We will use this plotting code several times, so put into a function for
# later use.

def plot_hgb_app():
    # Build plot, add custom label.
    hgb_app.plot.scatter('Hemoglobin', 'appetite_dummy',
                         c='appetite_color',
                         legend=True)
    plt.ylabel('Appetite\n0 = poor, 1 = good')
    plt.yticks([0,1]);  # Just label 0 and 1 on the y axis.
    # Put a custom legend on the plot.  This code is a little obscure.
    plt.scatter([], [], c='blue', label='good')
    plt.scatter([], [], c='red', label='poor')

# Do the plot
plot_hgb_app()
# Show the legend
plt.legend();
```

From the plot, it does look as if the patients with lower hemoglobin are more
likely to have poor appetite (`appetite_dummy` values of 0), whereas patients
with higher hemoglobin tend to have good appetite (`appetite_dummy` values of
1).

Now we start to get more formal, and develop a model with which we predict the
`appetite_dummy` values from the  `Hemoglobin` values.


## How about linear regression?


Remember that, in linear regression, we predict scores on the *outcome*
variable (or column) using a straight-line relationship of the *predictor*
variable (or column).

Why not use this same technique for our case?  After all, the `appetite_dummy`
values are just numbers (0 and 1), as are our `Hemoglobin` values.

Earlier in the textbook, we performed linear regression by using `minimize`, to
find the value of the slope and intercept of the line which gives the smallest
sum of the squared prediction errors.

Recall that, in linear regression:

$$
\text{predicted} = intercept + slope * \text{predictor_variable}
$$

*predicted* and *predictor variable* here are sequences of values, with one
value for each observation (row) in the dataset. In our case we have:

```{python}
print('Rows in CKD data', len(hgb_app))
```

"observations" (patients), so there will be the same number of scores on the
predictor variable (`Hemoglobin`), and the same number of predictions, in
`predicted`. By contrast, the slope and intercept are single values, defining
the line.

We used `minimize` to find the values of the slope and intercept which give the
"best" predictions.  So far, we have almost invariably defined the *best*
values for slope and intercept as the values that give the smallest sum of the
squared prediction errors.

$$
\text{prediction errors} = \text{actual variable - predicted}
$$


What would happen if we tried to use linear regression to predict the
probability of having good appetite, based on hemoglobin concentrations?

Let us start by grabbing the `ss_any_line` function from the [Using minimize
page](../mean-slopes/using_minimize.html).

```{python}
def ss_any_line(c_s, x_values, y_values):
    # c_s is a list containing two elements, an intercept and a slope.
    intercept, slope = c_s
    # Values predicted from these x_values, using this intercept and slope.
    predicted = intercept + x_values * slope
    # Difference of prediction from the actual y values.
    error = y_values - predicted
    # Sum of squared error.
    return np.sum(error ** 2)
```

The sum of squared prediction error, in linear regression, is our *cost*
function. When we have a good pair of (intercept, slope) in `c_s`, our function
is *cheap* - i.e. the returned value is small.  When we have a bad pair in
`c_s`, our function is *expensive* - the returned value is large.

If the value from `ss_any_line` is large, it means the line we are fitting does
not fit the data well. The purpose of linear regression is to find the
line which leads to the smallest cost.  In our case, the cost is the sum of the
squared prediction errors.

Let's use linear regression on the current example.  From looking at our plot above, we start with a guess of -1 for the intercept, and 0.1 for the slope.

```{python}
# The x (predictor) and y (to-be-predicted) variables.
hemoglobin = hgb_app['Hemoglobin']
appetite_d = hgb_app['appetite_dummy']
# Use minimize to find the least sum of squares solution.
min_lin_reg = minimize(ss_any_line, [-1, 0.1], args=(hemoglobin, appetite_d))
# Show the results that came back from minimize.
min_lin_reg
```

OK, so that looks hopeful. Using linear regression with `minimize` we found that the sum of squared prediction errors was smallest for a line with:

```{python}
# Unpack the slope and intercept estimates from the results object.
lin_reg_intercept, lin_reg_slope = min_lin_reg.x
# Show them.
print('Best linear regression intercept', lin_reg_intercept)
print('Best linear regression slope', lin_reg_slope)
```

The linear regression model we are using here is:

```
predicted_appetite_d = intercept + slope * hemoglobin
```

Specifically:

```{python}
predicted_lin_reg = lin_reg_intercept + lin_reg_slope * hemoglobin
predicted_lin_reg
```

Let's plot our predictions, alongside the actual data.  We plot the predictions
from linear regression in orange.

```{python tags=c("hide-cell")}
# Do the base plot of the hemoglobin and appetite_d.
plot_hgb_app()

# A new plot on top of the old.
plt.scatter(hemoglobin, predicted_lin_reg,
            label='LR prediction',
            color='orange')
# Another plot, to show the underlying line
fine_x = np.linspace(np.min(hemoglobin), np.max(hemoglobin), 1000)
fine_y = lin_reg_intercept + lin_reg_slope * fine_x
plt.plot(fine_x, fine_y, linewidth=1, linestyle=':')
# Show the legend.
plt.legend();
```

The linear regression line looks plausible, as far as it goes, but it has
several unhappy features for our task of predicting the `appetite_d` 0 / 1
values.

It looks like the predictions are getting it right that the value of
`appetite_d` is more likely to be 1 (meaning "good") at higher values of
`hemoglobin`.

The prediction line slopes upward as `hemoglobin` gets higher, indicating that
the probability of good appetite gets higher as the hemoglobin concentration
rises, across patients.

However, when the `hemoglobin` gets higher than about 15.5, linear regression
starts to predict a value for `appetite_d` that is greater than 1 - which, of
course, cannot occur in the `appetite_d` values, which are restricted to 0 or
1.

Looking at the plot, without the regression line, it looks as if we can be
fairly confident of predicting a 1 ("good") value for a hemoglobin above 12.5, but we are increasingly less confident about predicting a 1 value as hemoglobin drops down to about 7.5, at which point we become confident about predicting a 0 value.

These reflections make as wonder whether we should be using something other than a simple, unconstrained straight line for our predictions.


## Another prediction line


Here's another prediction line we might use for `appetite_d`, with the
predicted values.

For the moment, please don't worry about how we came by this line, we will come onto that soon.

The new prediction line is in gold.

```{python tags=c("hide-cell")}
# This is the machinery for making the sigmoid line of the plots below.  We
# will come on that machinery soon.  For now please ignore this code, and
# concentrate on the plots below.

def logit(y):
    """ Apply logit transformation
    """
    odds_ratios = y / (1 - y)
    return np.log(odds_ratios)


def inv_logit(y):
    """ Reverse logit transformation
    """
    odds_ratios = np.exp(y)  # Reverse the log operation.
    return odds_ratios / (odds_ratios + 1)  # Reverse odds ratios operation.


def params2pps(intercept_and_slope, x):
    """ Calculate predicted probabilities of 1 for each observation.
    """
    # store the intercept and slope as Python variables.
    intercept, slope = intercept_and_slope
    # Predicted log odds of being in class 1.
    predicted_log_odds = intercept + slope * x
    return inv_logit(predicted_log_odds)


nice_c_s = [-7, 0.8]  # Some plausible values for intercept and slope.
predictions_new = params2pps(nice_c_s, hemoglobin)
```

```{python tags=c("hide-cell")}
# Do the base plot of the hemoglobin and appetite_d.
plot_hgb_app()

# A new plot on top of the old.
plt.scatter(hemoglobin, predictions_new,
            label='New prediction',
            color='gold')
# Another plot, to show the underlying line
fine_y_sigmoid = params2pps(nice_c_s, fine_x)
plt.plot(fine_x, fine_y_sigmoid, linewidth=1, linestyle=':')
# Show the legend.
plt.legend();
```

The new not-straight line seems to have much to recommend it.  This shape of
line is called "sigmoid", from the name of the Greek letter "s".  The sigmoid
prediction here never goes above 1 or below 0, so its values are always in the
range of the `appetite_d` data it is trying to predict.  It climbs steeply to a
prediction of 1, and plateaus there, as we get to the threshold of hemoglobin
around 12.5, at which every patient does seem to have "good" appetite
(`appetite_d` of 1).

We can think of the values from the sigmoid curve as being *predicted
probabilities*.  For example, at a `hemoglobin` value of 10, the curve gives a
predicted y (`appetite_d`) value of about 0.73.  We can interpret this
prediction as saying that, with a hemoglobin value of 10, there is a
*probability* of about 0.73 that the corresponding patient will have a "good"
appetite (`appetite_d` value of 1).

Now let us say that we would prefer to use this kind of sigmoid line to predict
`appetite_d`.  So far, we have only asked `minimize` to predict directly from a
straight line - for example, in the `ss_any_line` function.   How can we get
minimize to predict from a family of sigmoid curves, as here?  Is there a way
of transforming a sigmoid curve like the one here, with y values from 0 to 1,
to a straight line, where the y values can vary from large negative to large
positive.  We would like to do such a conversion, so we have a slope and
intercept that `minimize` can work with easily.

The answer, you can imagine, is "yes" - we can go from our sigmoid 0 / 1 curve
to a straight line with unconstrained y values, in two fairly simple steps. The
next sections will cover those steps.  The two steps are:

* Convert the 0 / 1 *probability* predictions to 0-to-large positive
  predictions of the *odds ratio*.  The odds-ratio can vary from 0 to very
  large positive.
* Apply the *logarithm* function to convert the 0-to-very-large-positive
  odds-ratio predictions to log odds-ratio predictions, which can vary from
  very large negative to very large positive.

These two transformations together are called the *log-odds* or
[logit](https://en.wikipedia.org/wiki/Logit) transformation.  *Logistical
regression* is regression using the *logit* transform.  Applying the logit
transform converts the sigmoid curve to a straight line.

We will explain more about the two stages of the transform below, but for now, here are the two stages in action.

This is the original sigmoid curve above, with the predictions, in its own
plot.

```{python tags=c("hide-cell")}
plt.scatter(hemoglobin, predictions_new,
            color='gold')
plt.plot(fine_x, fine_y_sigmoid, linewidth=1, linestyle=':')
plt.title('Sigmoid probability prediction')
plt.xlabel('Hemoglobin')
plt.ylabel('Probability prediction');
```

Next we apply the conversion from probability to odds-ratio:

```{python tags=c("hide-cell")}
predictions_or = predictions_new / (1 - predictions_new)
plt.scatter(hemoglobin, predictions_or,
            color='gold')
fine_y_or = fine_y_sigmoid / (1 - fine_y_sigmoid)
plt.plot(fine_x, fine_y_or, linewidth=1, linestyle=':')
plt.title('Odds-ratio prediction')
plt.xlabel('Hemoglobin')
plt.ylabel('Odds-ratio');
```

Notice that this is an *exponential* graph, where the y values increase more
and more steeply as the x values increase.  We can turn exponential lines like
this one into straight lines, using the *logarithm* function, the next stage of
the logit transformation:

```{python}
predictions_or_log = np.log(predictions_or)
plt.scatter(hemoglobin, predictions_or_log,
            color='gold')
fine_y_or_log = np.log(fine_y_or)
plt.plot(fine_x, fine_y_or_log, linewidth=1, linestyle=':')
plt.title('Logit prediction')
plt.xlabel('Hemoglobin')
plt.ylabel('Log odds-ratio');
```

Now we have a straight line, with an intercept and slope, suitable for
`minimize`.  The next few sections go into more detail on the odds-ratio and
logarithm steps.

## Probability and Odds


For logistic regression, in contrast to linear regression, we are interested in
predicting the *probability of an observation falling into a particular outcome
class* (0 or 1).

In this case, we are interested in the probability of a patient having "good"
appetite, predicted from the patient's hemoglobin.

We can think of probability as the *proportion of times* we expect to see a
particular outcome.

For example, there are 139 patients with "good" appetite in this data frame,
and 158 patients in total.  If you were to repeatedly draw a single patient at
random from the data frame, and record their `Appetite`, then we expect the proportion of "good" values in the long run, to be 139 / 158 --- about 0.88.  That is the same as saying there is a probability of 0.88 of a randomly-drawn patient of having a "good" appetite.

Because the patient's appetite can only be "good" or "poor", and because the
probabilities of all possible options have to add up to 1, the probability of
the patient having a "poor" appetite is 1 - 0.88 --- about 0.12.

So, the probability can express the *proportion of times* we expect to see some
*event of interest* - in our case, the event of interest is "good" in the
`Appetite` column.

We can think of this same information as an *odds ratio*.

We often express probabilities as odds ratios.  For example, we might say that
the odds are 5 to 1 that it will rain today.   We mean that it is five times
more likely that it will rain today than that it will not rain today.   On
another day we could estimate that the odds are 0.5 to 1 that it will rain
today, meaning that it is twice as likely *not* to rain, as it is to rain.

The odds ratio is the number of times we expect to see the event of interest
(e.g. rain) for every time we expect to see the event of no interest (not
rain).

This is just a way of saying a probability in a different way, and we can convert easily between probabilities and odds ratios.

To convert from a probability to an odds ratio, we remember that the odds ratio is the number of times we expect to see the event of interest for every time we expect to see the event of no interest.   This is the probability (proportion) for the event of interest, divided by the probability (proportion) of the event of no interest.  Say the probability p is some value (it could be any value):

```{python}
# Our proportion of interest.
p = 0.88
```

Then the equivalent odds ratio `odds_ratio` is:

```{python}
# Odds ratio is proportion of interest, divided by proportion of no interest.
odds_ratio = p / (1 - p)
odds_ratio
```

Notice the odds ratio is greater than one, because `p` was greater than 0.5. An
odds ratio of greater than one means the event of interest is more likely to
happen than the alternative.  An odds ratio of less than one means p was less
than 0.5, and the event of interest is less likely to happen than the
alternative.   A p value of 0 gives an odds ratio of 0.  A p value of 1 gives:

```{python tags=c("raises-exception")}
1 / (1 - 1)
```

This is either an error (as it is here when we do the calculation with Python's standard integers) or Infinity, when we do the calculation with Numpy:

```{python}
one = np.array(1)  # Make an Numpy array containing 1
one / (one - one)
```

We can also convert from odds ratios to p values.  Remember the odds ratio is
the number of times we expect to see the event of interest, divided by the
number of times we expect to see the event of no interest.  The probability is
the proportion of all events that are events of interest.  We can read an
`odds_ratio` of - say - 2.5 as "the chances are 2.5 to 1".  To get the
probability we divide the number of times we expect to see the event of
interest - here 2.5 - by the number of events in total.  The number of events
in total is just the number of events of interest - here 2.5 - plus the number
of events of no interest - here 1.  So:

```{python}
p_from_odds_ratio = odds_ratio / (odds_ratio + 1)
p_from_odds_ratio
```

Summary - convert probabilities to odds ratios with:

```{python}
odds_ratio = p / (1 - p)
```

Convert odds ratios to probabilities with:

```{python}
p = odds_ratio / (odds_ratio + 1)
```

As you've seen, when we apply the conversion above, to convert the probability values to odds ratios for our appetite predications, we get the following:

```{python tags=c("hide-cell")}
predictions_or = predictions_new / (1 - predictions_new)
plt.scatter(hemoglobin, predictions_or,
            color='gold')
fine_y_or = fine_y_sigmoid / (1 - fine_y_sigmoid)
plt.plot(fine_x, fine_y_or, linewidth=1, linestyle=':')
plt.title('Odds-ratio prediction')
plt.xlabel('Hemoglobin')
plt.ylabel('Odds-ratio');
```

Notice that the odds ratios we found vary from very close to 0 (for p value
predictions very close to 0) to very large (for p values very close to 1).

Notice too that our graph looks exponential, and we want it to be a straight
line.   Our next step is to apply a *logarithm* transformation.


# A quick refresher on logarithms

Don't worry - you don't need to do anything fancy with logarithms here.  You
don't even need to fully understand this quick refresher.  In practice, all you
need to know about logarithms for this section is that they make straight line graphs from exponential graphs.

That said, here is a whirlwind tour of logarithms.

Logarithms ask the question of a number - "what exponent do I need in order to
make that number"?

For example, consider the number:

```{python}
x = 100
```

I want to write that number as 10 to the power of something - $10^y$.  $y$ is
the *exponent* that I need for 10, to make 100.  In this case I can see that
$y$ must be 2, because $10^2 = 100$.  If $x = 1000$ then $y = 3$ because $10^3
= 1000$. The function `np.log10` works this out for us.  It takes the numbers
we send it, and works out the exponent(s) it needs to apply to 10, to get those
numbers.  For example:

```{python}
np.log10([100, 1000])
```

The exponents don't have to be whole numbers.  For example, I can calculate $10^{3.5}$:

```{python}
v = 10 ** 3.5
v
```

`np.log10` will find this exponent for me too:

```{python}
np.log10(v)
```

and:

```{python}
L = np.log10([100, 1000, v])
L
```

`np.log10` reverses the effect of calculating 10 to the power of some value.

```{python}
values = np.array([2, 3, 4])
v_exp_10 = 10 ** values
v_exp_10
```

```{python}
# log10 reverses the effect of 10 to the power of some values, to get those
# values back.
np.log10(v_exp_10)
```

Similarly, if I've transformed some values with `np.log10`, I can reverse that
transformation by taking 10 to the power of the transformed values:

```{python}
more_values = np.array([10, 15, 20])
v_log_10 = np.log10(more_values)
v_log_10
```

```{python}
# Taking 10 to the power of some values, reverses the effect of log10.
10 ** v_log_10
```

You may remember that powers can be less than 1.  For example $10^{0.5}$ is the
square root of 10:

```{python}
print(np.sqrt(10))
print(10 ** 0.5)
```

Therefore:

```{python}
np.log10(np.sqrt(10))
```

It is a bit difficult to think about what $10^0$ might mean, but the rule is
that $10^0$ is equal to 1, and therefore log10 of 1 is 0:

```{python}
np.log10(1)
```

Exponents can be negative.  A negative exponent gives the equivalent calculation to taking the power to the exponent without the minus sign, and then dividing into 1 - like this:

```{python}
print('10 ** -2 equals', 10 ** -2)
print('1 / 10 ** 2 equals', 1 / (10 ** 2))
```

Therefore, a log of a number less than 1 will be negative:

```{python}
np.log10([0.5, 0.1, 0.01])
```

But - logarithms don't know how to handle minus numbers.  For example, there is
no exponent you can apply to 10 to get -100:

```{python}
np.log10(-100)
```

So far all our examples of logs have been calculating the exponents for 10.
This is what `np.log10` does.  10 is called the *base* of the logarithm - the
number that we are calculating the exponent for. `np.log10` calculates
logarithms with base 10.

The base doesn't have to be 10.  Another common option is `np.log2` where we calculate the exponent we have to apply to 2, to get the input numbers:

```{python}
np.log2([2, 4, 8, 10])
```

In fact an even more common option is to use the special number $e$ as the
base.  This is because taking exponents or calculating logarithms with base $e$
have some very convenient mathematical properties, that are not relevant to us
here.  Log to the base e is so common that Numpy simply uses `np.log` to mean
logarithm to base $e$.

```{python}
print('e', np.e)
print('e squared', np.e ** 2)
print('e cubed', np.e ** 3)
```

As you would expect, `np.log` returns the exponent we need to apply to $e$ to recreate the input numbers:

```{python}
np.log([np.e, np.e ** 2, np.e ** 3])
```

As for all log bases, log of 1 is 0, and numbers less than 1 give negative log
values:

```{python}
np.log([1, 0.5, 0.1])
```

## A different measure of prediction error

To address this issue, logistic regression does not minimize the sum of the squared prediction errors like linear regression does. 

Logistic regression uses a different function for calculating prediction errors, and it minimizes this function instead. Another way of saying this is that it minimizes a different cost function.

The cost function that logistic regression uses is convex - meaning it works much better with minimize, as it has a single lowest point.

To understand the function logistic regression uses to calculate prediction errors, we will need to do a very brief aside about <i> natural logarithms </i>.

#### Natural Logarithms

The natural logarithm of a number tells you: "what power would I have to raise `e` to, in order to produce this number?".

This sounds more complex than it is. 

`e` squared equals 7.3890560989306495, as we can see from the cell below:
<!-- #endregion -->

```{python}
np.e**2
```

If I asked you, now you've seen the cell above: "what number would I have to raise `e` to, in order to get 7.3890560989306495?".

If you're really good at remembering decimal places, you'd say: "well, `e` squared equals 7.3890560989306495. So you raise `e` to the power of 2 to get 7.3890560989306495".

Another way of saying this is that the natural log of 7.3890560989306495 is 2. 

We can represent this with a formula:

$$ ln(7.3890560989306495) = 2 $$

`ln(7.3890560989306495)` means 'the natural log of 7.3890560989306495'. We can use `np.log()` to calculate the natural logarithm of a number.

```{python}
np.log(7.3890560989306495)
```

In logistic regression, we calculate the prediction errors are follows:

If the dummy variable of the actual outcome score equals 1, then the prediction error equals the negative natural logarithm of the predicted probability:

 $$ \text{if the actual outcome == 1, then:}$$
 $$\text{prediction error = -ln(predicted probability)} $$

If the dummy variable of the actual outcome score equals 0, then the prediction error equals the negative natural logarithm of 1 minus the predicted probability.

 $$ \text{if the actual outcome == 0, then:}$$
 $$\text{prediction error = -ln(1 - predicted probability)} $$

Again, this looks more complex than it is (I promise!). Once more, it is better to focus on how this calculation works, rather than why it works.

The table below shows some possible pairings of the actual outcome and predicted outcome. The predicted outcome is a probability, it is the predicted probability that an observations falls into the class with the dummy variable 1:

```{python}
# this is just here to generate the illustration

illustration = pd.DataFrame({'actual outcome': [0, 1, 0, 1], 'predicted probability of being in class 1': [0.2, 0.8, 0.8,0.2],
                           'prediction close?': ['yes', 'yes', 'no', 'no']})

illustration
```

Let's write a python function to calculate the prediction errors for logistic regression. 

This is called a <i> piecewise function, </i> because what the function does varies depending on what input it is given. In python, we can do this with `if` statements:

```{python}
def log_reg_pred_err(actual_outcome, predicted_probability):
    
    # create an array to store the prediction errors
    prediction_error = np.zeros(len(actual_outcome))
    
    # for every observation
    for i in np.arange(len(actual_outcome)):
        
        # if the actual outcome for that observation is in class 1...
        if actual_outcome[i] == 1:
            
            # ...then the prediction error equals the negative natural logarithm of the predicted probability
            prediction_error[i] = -np.log(predicted_probability[i])
            
        # if the actual outcome for that observation is in class 0...
        if actual_outcome[i] == 0:
            
            # ...then prediction error equals the negative natural logarithm of 1 minus the predicted probability
            prediction_error[i] = -np.log(1 - predicted_probability[i])
        
    return prediction_error
```

Let's calculate the prediction errors for the values in the dataframe above:

```{python}
illustration['prediction error'] = log_reg_pred_err(illustration['actual outcome'], 
                                                    illustration['predicted probability of being in class 1'])

illustration
```

Make sure you inspect this dataframe in great detail. It is very important. Remember that the predicted probability is is the probability of being in class 1. 

We want the predicted probability to be large (close to 1) when the actual outcome score is in class 1. We want the predicted probability to be small (close to 0) when the actual outcome score is in class 0.

<b> You can see that when the prediction is close - i.e. when the predicted probability is accurate - the prediction error is small. </b>

<b> When the prediction is not close, the prediction error is large. </b>

Remember that good model leads to <i> small prediction errors </i>. 

Even if you do not understand the precise mechanics of the prediction error formulas in logistic regression, you can see how they work. 

<i> When the predictions are good, the error values are small. When the predictions are bad, the error values are large. </i>

Let's illustrate this further with a graph. The graph below shows the prediction error function for logistic regression:

```{python}
# do not worry about this code, it just generates the graph

actual = np.random.choice([0,1], size = 5000)

predictions = np.random.uniform(0.01, 0.99, size = 5000)

error = log_reg_pred_err(actual, predictions)

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize = (10,8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(actual, predictions, error)
plt.xlabel('Actual Outcome')
plt.xticks([0,1])
plt.ylabel('Predicted probability of being in class 1')

ax.set_zlabel('Prediction Error');
plt.show()
```

<!-- #region -->
Again, take a bit of time to go over this graph. It is very important for understanding logistic regression.

You can see from the graph that:

If the actual outcome class is 0, and the predicted probability is close to 0, the prediction error is low.

If the actual outcome class is 0, and the predicted probability is close to 1, the prediction error is high.

If the actual outcome class is 1, and the predicted probability is close to 0, the prediction error is high.

If the actual outcome class is 1, and the predicted probability is close to 1, the prediction error is low.

Keep re-visiting the graph until you are sure of this pattern.

## What is the name of the function for calculating prediction errors in logistic regression?

The function we have just used for calculating the prediction errors in logistic regression is called the <i> cross-entropy cost function. </i>

Conversely the errors in linear regression are calculated using the <i> squared prediction error function </i>.

Both of these are shown again below, just so you can see how different they are:

### Logistic regression cross-entropy cost function:

 $$ \text{if the actual outcome == 1, then:}$$
 $$\text{prediction error = -ln(predicted probability)} $$

 $$ \text{if the actual outcome == 0, then:}$$
 $$\text{prediction error = -ln(1 - predicted probability)} $$
 
### Linear regression squared prediction error function:

$$ \text{squared prediction error = (actual outcome value - predicted outcome value)}^2 $$


<!-- #endregion -->


## What do the slope and intercept mean for logit

Changing the intercept of the logit straight line moves the corresponding
sigmoid curve left and right on the horizontal axis:

```{python}
for intercept in [-6, -7, -8]:
    plt.plot(fine_x,
            params2pps([intercept, 0.8], fine_x),
            linewidth=1,
            linestyle=':',
            label='logit intercept=%d' % intercept)
plt.title('Sigmoid probability predictions')
plt.xlabel('Hemoglobin')
plt.ylabel('Probability prediction')
plt.legend();
```

Changing the slope of the logit straight line makes the transition from 0 to 1
flatter or steeper:

```{python}
for slope in [0.6, 0.8, 1.0]:
    plt.plot(fine_x,
            params2pps([-7, slope], fine_x),
            linewidth=1,
            linestyle=':',
            label='logit slope=%.1f' % slope)
plt.title('Sigmoid probability predictions')
plt.xlabel('Hemoglobin')
plt.ylabel('Probability prediction')
plt.legend();
```

# Logistic Regression using minimize

We now have the tools we need to implement logistic regression using minimize. We are going to predict Anemia status (yes/no) from Red Blood Cell Count.

The actual function that we minimize in logistic regression is the <i> average prediction error, </i> where each prediction error is calculated using the cross-entropy function shown above.

Let's write a function to pass to `minimize`, which calculates the average prediction error for an intercept and slope pair. Go over the comments in the cell below and make sure you understand what each line is doing.

```{python}
def logb(arr, b=np.e):
    """ Logarithm of `arr` to base `b`
   
    From formula in:
    
    https://en.wikipedia.org/wiki/Logarithm#Change_of_base
    """
    return np.log(arr) / np.log(b)
```

```{python}
print(logb(11), np.log(11))
print(logb(11, b=10), np.log10(11))
print(logb(11, b=2), np.log2(11))
```

```{python}
def params2pps(intercept_and_slope, x, b=np.e):
    """ Calculate predicted probabilities of 1 for each observation
    """
    # store the intercept and slope as Python variables.
    intercept, slope = intercept_and_slope
    # Predicted log odds of being in class 1.
    predicted_log_odds = intercept + slope * x
    # To get the odds, we invert the log operation.
    predicted_odds = b ** predicted_log_odds
    # Predicted probability of being in class 1.
    return predicted_odds / (1 + predicted_odds)
```

```{python}
def p_y_given_pps(pps, y):
    # pps is the predicted probability of being in class 1.
    # Because class 1 and 0 are mutually exclusive:
    pp0 = 1 - pps
    p_of_y = pps.copy()  # y == 1 values correctly set.
    p_of_y[y == 0] = pp0[y == 0]  # set the y==0 values.
    # Or - equivalently, but rather less readably:
    # p_of_y = y * pp1 + (1 - y) * pp0
    return p_of_y
```

```{python}
def calc_log_likelihood(pp1, y, b=np.e):
    """ Overall log likelihood of `y` given predicted ps `pp1`
    """
    p_of_ys = p_y_given_pps(pp1, y)
    # Overall likelihood of this intercept, slope combination is the product of
    # all the likelihoods (we are multiplying probabilities).
    # That would be:
    # likelihood = np.prod(likelihoods)
    # But - if we just multiply these, there are often so many values close to
    # zero that the result gets close to the smallest values the computer can
    # represent or even gets smaller, in which case the value becomes 0.
    # To avoid this, we can add the logs (instead of taking the product of the
    # original values).  Adding the logs gives us the log of the likelihood as
    # defined above.
    # Which base we use for the log here does not matter, we're just
    # trying to maintain the precision, the results will go up and down
    # in the same way, for the same values, and we don't care about
    # the exact number, only that it is higher for a higher likelihood.
    # But - just for completeness, use the general log function.
    log_likelihood = np.sum(logb(p_of_ys, b=b))
    return log_likelihood
```

```{python}
def log_reg_cost(intercept_and_slope, x, y, b=np.e):
    # Predicted probability of being in class 1.
    pp1 = params2pps(intercept_and_slope, x, b=b)
    # Overall log of the likelihood of observed outomes y
    log_likelihood = calc_log_likelihood(pp1, y, b=b)
    # At the moment, a higher value here means the intercept and slope are a
    # *better* fit, but minimize wants the value to go *down* for a better fit,
    # so we just stick a minus on it to make better fits give lower values.
    return -log_likelihood
```

```{python}
from scipy.optimize import minimize

predictor = hgb_app['Hemoglobin']
outcome = hgb_app['appetite_dummy']

# Results using default base of np.e
b_e_res = minimize(log_reg_cost, [1,1], args = (predictor,outcome))
p_values_e = params2pps(b_e_res.x, predictor)
print('Base e fun value, best-fit parameters:', b_e_res.fun, b_e_res.x)
# Results using base 10
b_10_res = minimize(log_reg_cost, [1,1], args = (predictor, outcome, 10))
p_values_10 = params2pps(b_10_res.x, predictor, b=10)
print('Base 10 fun value, best-fit parameters:', b_10_res.fun, b_10_res.x)
# The fun value, parameters for log e, log 10 are predictably scaled
# versions of each other.
print('Base 10 fun value, parameters, scaled by log(10):',
      b_10_res.fun * np.log(10), b_10_res.x * np.log(10))
# The estimated p values are (more or less) the same.
print('p likelihoods similar?', np.allclose(p_values_e, p_values_10))
```

```{python}
# Review what happens with the likelihood.
pp1 = params2pps(b_e_res.x, predictor)
p_of_ys = p_y_given_pps(pp1, outcome)
p_of_ys
```

```{python}
# We would be sort-of OK with the best parameters, in that
# the product of the p values is not that close to 0:
likelihood = np.prod(p_of_ys)
print(likelihood)
log_likelihood = np.sum(np.log(p_of_ys))
print(log_likelihood)
print(np.e ** log_likelihood)
```

```{python}
# But it's a mess if we are further off, so some of the p
# values get very small, and the product of the p values gets
# very close to 0 - and then so close that it becomes 0.
pp1_bad = params2pps([12, -1], predictor)
p_of_ys_bad = p_y_given_pps(pp1_bad, outcome)
p_of_ys_bad
```

```{python}
likelihood_bad = np.prod(p_of_ys_bad)
print(likelihood_bad)
log_likelihood_bad = np.sum(np.log(p_of_ys_bad))
print(log_likelihood_bad)
print(np.e ** log_likelihood_bad)
```

```{python}
# We could fix this with numbers that have 50 digits of precision
# but that would be horribly slow.
from decimal import Decimal, getcontext
getcontext().prec = 50  # 50 decimal points.
# An array of numbers working with 50 decimal digits of precision.
p_of_ys_bad_hp = p_of_ys_bad.apply(Decimal)
likelihood_bad_hp = np.prod(p_of_ys_bad_hp)
print(likelihood_bad_hp)
logs_hp = p_of_ys_bad_hp.apply(Decimal.ln)
log_likelihood_bad_hp = np.sum(logs_hp)
print(log_likelihood_bad_hp)
e_hp = Decimal(1).exp()  # e to 50 decimal places.
print(e_hp ** log_likelihood_bad_hp)
```

Notice above that the log likelihood is near as dammit the same as when we do the calculation without the high-precision numbers, but the high-precision numbers stop the rounding to 0.

The conclusion is - we can keep using the log-likelihood at our usual fast precision, but we do need to do the log-likelihood rather than the direct product of p values, to avoid the calculation breaking down when it's trying parameters that are fairly far off.


Now let's pass our function to `minimize`, to find the slope and intercept which minimize the average prediction error.

We'll start with an intercept of 1 and a slope of 1 as our first guess. 

```{python}
# store the predictor variable as a python variable
predictor = hgb_app['Hemoglobin']

# store the dummy-coded outcome variable as a python variable 
outcome = hgb_app['appetite_dummy']

min_log_reg = minimize(log_reg_cost, [1,1], args = (predictor,outcome))
min_log_reg 
```

So `minimize` found an intercept of 10.43827715 and a slope of -3.14932093. These values minimize the prediction errors, calculated using the cross-entropy function.

The slope is more interesting than the intercept here. 

In fact the intercept is quite hard to interpret (for those who are interested: it is is the predicted natural logarithm of the odds of being in outcome class 1, for an observation with a Red Blood Cell Count of 0).

The precise value of the slope is also quite hard to interpret, because it also relates to the natural logarithm of the odds of being in outcome class 1. 

However, because the slope is negative, this tells us that as Red Blood Cell Count increases, the odds of being in class 1 (having anemia) decrease.

Interpreting logistic regression is much easier if we use this slope and intercept to generate predicted probabilities of having anemia, for each observation.

As we did before, let's generate the predicted probabilities using the intercept/slope values that `minimize` has found. The formulas for doing this are shown once more here for convenience:

Calculate the predicted odds of having anemia:

$$ \text{predicted odds of anemia} = e^{\text{intercept + slope * Red Blood Cell Count}} $$

Calculate the predicted probability of having anemia:

$$ \text{predicted probability of anemia} = \frac{e^{\text{intercept + slope * Red Blood Cell Count}}}{1 + e^{\text{intercept + slope * Red Blood Cell Count}}} = \frac{e^{\text{odds of anemia}}}{1 + e^{\text{odds of anemia}}} $$

<i> Remember: </i> there is one predicted probability for each observation. This predicts the probability of having anemia for that patient, based on the patient's score on the Red Blood Cell Count variable.

```{python}
# store the intercept as a python variable
log_reg_intercept = min_log_reg.x[0]

# store the slope as a python variable
log_reg_slope = min_log_reg.x[1]

# generate the predicted odds of having anemia (using the tricks from above)
predicted_odds_log_reg = np.e**(log_reg_intercept + log_reg_slope * hgb_app['Hemoglobin'])

# generate the predicted probability of having anemia (using the tricks from above)
predicted_probs_log_reg = predicted_odds_log_reg/(1 + predicted_odds_log_reg)

predicted_probs_log_reg
```

Let's plot the predictions against the actual data:

```{python}
# do not worry about this code, it just generates the graph
plt.figure(figsize = (10, 4))
plt.scatter(hgb_app['Hemoglobin'], hgb_app['appetite_dummy'], label = 'not anemic')
plt.scatter(hgb_app[hgb_app['Appetite'] == 'yes']['Hemoglobin'], 
                             hgb_app[hgb_app['Appetite'] == 'yes']['appetite_dummy'], color = 'red',
            label = 'anemic')
plt.scatter(hgb_app['Hemoglobin'], predicted_probs_log_reg, 
            label = 'predicted probability of anemia (logistic regression)', color = 'green')
plt.xlabel('Hemoglobin')
plt.ylabel('Anemia \n(0 = no, 1 = yes)')
plt.legend()
plt.yticks([0,0.5, 1]);
```

Just for comparison, let's also show the predictions we got when we predicted the probabilities using the slope and intercept that we got from linear regression:

```{python}
# do not worry about this code, it just generates the graph
plt.figure(figsize = (10, 4))
plt.scatter(hgb_app['Hemoglobin'], hgb_app['appetite_dummy'], label = 'not anemic')
plt.scatter(hgb_app[hgb_app['Appetite'] == 'yes']['Hemoglobin'], 
                             hgb_app[hgb_app['Appetite'] == 'yes']['appetite_dummy'], color = 'red',
            label = 'anemic')
plt.scatter(hgb_app['Hemoglobin'], predicted_probs_log_reg, 
            label = 'predicted probability of anemia (logistic regression)', color = 'green')
plt.scatter(hgb_app['Hemoglobin'], prob_anemia, 
            label = 'predicted probability of anemia (linear regression)', color = 'orange')
plt.xlabel('Hemoglobin')
plt.ylabel('Anemia \n(0 = no, 1 = yes)')
plt.legend()
plt.yticks([0,0.5, 1]);
```

We can see that logistic regression fits the data much better.

With logistic regression, the predicted probability of having anemia falls steeply as the Red Blood Cell Count increases.

The predicted probabilities from linear regression overestimate the probability of anemia at higher values of Red Blood Cell Count.

You can see that the predictions from logistic regression form an 'S' shape. This is a nonlinear function called a <i> sigmoid function. </i>

Linear regression fits a <i> line to directly predict outcome scores, based on scores on a predictor variable.</i>

Logistic regression fits a <i> sigmoid function to predict the probability of being in one of two categories, based on scores on a predictor variable </i>.

## Logistic Regression with statsmodels

As with linear regression, we can easily perform logistic regression using statsmodels.

To do this, we first have to add a column of 1's to the predictor variable. This is to ensure the intercept is fitted correctly, though we will not explore why - just remember to do it in order to get better estimates! We can use the statsmodels function `add_constant()` to do this:

<i> Note: here we are using `statsmodels.api` rather than `statsmodels.formula.api` </i> 

```{python}
list(df)
```

```{python}
import statsmodels.formula.api as smf
```

```{python}
# Creating the model
op_col = 'Appetite'
val_1 = 'good'
ip_col = 'Hemoglobin'
ip_df = df.loc[:, [op_col, ip_col]]
ip_df['dummy'] = (ip_df[op_col] == val_1).astype(int)
ip_df.plot.scatter(ip_col, 'dummy')
```

```{python}
col_is_obj = df.dtypes == object
category_cols = df.columns[col_is_obj]
num_cols = df.columns[~col_is_obj]
nums = df.replace({'yes': 1, 'no': 0,
                   'abnormal': 1, 'normal': 0,
                   'present': 1, 'notpresent': 0,
                   'good': 1, 'poor': 0
                  }).astype(np.float)
nums.corr().loc[category_cols, num_cols]
```

```{python}
log_reg_mod = smf.logit('dummy ~ Q("{}")'.format(ip_col),
                        data=ip_df)

fitted_log_reg_mod = log_reg_mod.fit() # fitting the model

fitted_log_reg_mod.summary() # showing the model summary
```

Look at the table above under 'coef'. Compare the logistic regression intercept and slope that statsmodels has obtained to the ones we obtained through using `minimize`:

```{python}
print('Intercept from minimize =', log_reg_intercept)
print('Slope from minimize =', log_reg_slope)
```

Finally, we can use the `predict` method of statsmodels to generate predicted
probabilities from the logistic regression model we just fitted:

```{python}
sm_predictions = fitted_log_reg_mod.predict(ip_df[ip_col])
sm_predictions
```

Let's plot the predicted probabilities of having anemia, from statsmodels:

```{python}
# do not worry about this code, it just generates the graph
plt.figure(figsize = (10, 4))
plt.scatter(hgb_app['Hemoglobin'], hgb_app['appetite_dummy'], label = 'not anemic')
plt.scatter(hgb_app[hgb_app['Appetite'] == 'yes']['Hemoglobin'], 
                             hgb_app[hgb_app['Appetite'] == 'yes']['appetite_dummy'], color = 'red',
            label = 'anemic')
plt.scatter(hgb_app['Hemoglobin'], sm_predictions,
            label = 'predicted probability of anemia (statsmodels logistic regression)', color = 'cyan')
plt.xlabel('Hemoglobin')
plt.ylabel('Anemia \n(0 = no, 1 = yes)')
plt.legend()
plt.yticks([0,0.5, 1]);
```

We can see graphically that these predictions look identical to the ones we obtained from minimize.

Let's see what the largest absolute difference between the predictions from the two methods is:

```{python}
np.max(np.abs(predicted_probs_log_reg - sm_predictions))
```

That is very close to 0. The models are making almost identical predictions.

## Summary

This tutorial has shown you how to do binary logistic regression with one numerical predictor variable.
